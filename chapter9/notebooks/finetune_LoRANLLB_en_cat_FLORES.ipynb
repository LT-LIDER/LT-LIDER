{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intoduction\n",
        "\n",
        "The student will fine-tune an open-source multiligual NMT (mNMT) model for low-resource language pairs (LRL), such as English-Catalan. The translation performance of the fine-tuned model should be compared against the original baseline using automatic evaluation metrics. For more information on mNMT, we direct the reader to **Chapter 16**.\n",
        "\n",
        "We will be using the [NLLB](https://huggingface.co/facebook/nllb-200-distilled-600M) architecture for fine-tuning. Fine-tuning is a critical process where we take the model's weights, pre-trained on a vast, diverse corpus, and incrementally adjust them using our LRL parallel corpus. This procedure allows the NLLB model to better capture the unique stylistic, lexical, and grammatical of our LRL pair, improving its translation quality.\n",
        "\n",
        "The Jupyter Notebook environment is the platform for generating and distributing documents that integrate live code, output, and explanatory text. For tasks in machine translation, it provides a streamlined, document-centric workspace ideal for rapid prototyping and experimentation. The architecture divides content into two primary components: documentation blocks (text/Markdown cells) for methodology descriptions and analysis interpretations, and execution blocks (code cells) for implementing and running NMT models and data processing routines (e.g., using Python).\n",
        "\n",
        "\n",
        "## Questions\n",
        "\n",
        "- Beyond automatic metrics scores (e.g. BLEU, chrF), what specific differences in translation output would you expect to see between the baseline, and fine-tuned models, particularly in terms of accuracy, fluency, and terminology? For more information on manual evaluation, we direct the reader to **Chapter 10**.\n",
        "- How might the performance observed when comparing the baseline, and fine-tuned models shift when working with a LRL pair, and what unique challenges or opportunities might each approach present?\n",
        "- Considering the computational resources and data requirements associated with each approach (baseline, fine-tuning), given project constraints, how would you prioritise the use of one model over the others, based on automatic metrics scores?\n"
      ],
      "metadata": {
        "id": "ONgvtFTU_sZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries\n",
        "\n",
        "To start the project, we must first install the required Python packages for our multilingual NMT workflow. We will specifically use resources like the Hugging Face Transformers library ([transformers](https://huggingface.co/docs/transformers/en/index)), which delivers state-of-the-art NMT models, allowing us to quickly implement, evaluate, and modify models without having to train them from scratch.\n",
        "\n",
        "The installation of these packages will be executed using pip install [package_name] within the code cells."
      ],
      "metadata": {
        "id": "JMmtLbd_chaF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvMMFlaBEsMc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]==4.57.1\n",
        "!pip install datasets==4.0.0\n",
        "!pip install evaluate==0.4.6\n",
        "!pip install accelerate==1.11.0\n",
        "!pip install sacrebleu==2.5.1\n",
        "!pip install bitsandbytes==0.48.2\n",
        "!pip install peft==0.17.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list installed libraries\n",
        "#this can use useful if code is not reproducible\n",
        "!pip list"
      ],
      "metadata": {
        "id": "XmvQ-WTzGQA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Processing\n",
        "\n",
        "To prepare the data for the mNMT model, we will perform a data preprocessing step: converting the raw source language and target language text files into the JSON format. This conversion is necessary because many state-of-the-art NMT frameworks (like those in Hugging Face), require data to be structured in a specific, machine-readable format—often a list of dictionaries or similar JSON structure to facilitate efficient tokenization, batching, and loading by the model's data loader utility.\n",
        "\n",
        "To proceed with multilingual NMT, the initial step is to upload our data. Please upload the raw English (source: *eng_Latn.dev*) and Catalan (target: *cat_Latn.dev*) text files—which represent the language pair extracted from the dev split of the [FLORES-200](https://github.com/facebookresearch/flores/tree/main/flores200) benchmark as our corpus into the Colab file system. Use the file upload buttom (found in the sidebar) within the notebook interface. Once uploaded, the files will reside in a temporary storage, making them accessible via the local file path for our code.\n",
        "\n",
        "**NOTE**: Please pay attention to the in-line documentation. Explanations regarding the function, and specific NMT techniques employed within the code blocks are provided through Python comments, which are defined by the hash symbol (#). Reading these comments is important for understanding why specific steps are being taken in the NMT workflow."
      ],
      "metadata": {
        "id": "uP0VOdJRcZ99"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70uOsNJMMnce"
      },
      "outputs": [],
      "source": [
        "#txt to json\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "import codecs\n",
        "\n",
        "# json from huggingface example output file:\n",
        "#{ \"translation\": { \"eng_Latn\": \"The pilot was identified as Squadron Leader Dilokrit Pattavee\", \"cat_Latn\": \"Es va identificar el pilot com el líder de l'esquadró Dilokrit Pattavee.\" } }\n",
        "#{ \"translation\": { \"eng_Latn\": \"Vettel, on the other hand, hasn't won since 2013, when he was a Red Bull driver.\", \"cat_Latn\": \"Vettel, en canvi, no hi guanya des del 2013, quan era pilot de Red Bull.\" } }\n",
        "#...\n",
        "\n",
        "def txt2json(src_id, trg_id, src_file, trg_file, out_file):\n",
        "  \"\"\"\n",
        "  Read source and target files with utf-8 encoding\n",
        "  and write into output json structured file\n",
        "  \"\"\"\n",
        "\n",
        "  src = codecs.open(src_file, 'r', encoding=\"utf-8\")\n",
        "  trg = codecs.open(trg_file, 'r', encoding=\"utf-8\")\n",
        "  #output file\n",
        "  out_json = codecs.open(out_file, 'w', encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "  src_lines = []\n",
        "  trg_lines = []\n",
        "  for line_s, line_t in zip(src, trg):\n",
        "      line_s = line_s.strip()\n",
        "      line_t = line_t.strip()\n",
        "      src_lines.append(line_s)\n",
        "      trg_lines.append(line_t)\n",
        "  recs = [src_lines, trg_lines]\n",
        "  for src, tgt in zip(*recs):\n",
        "      #print the json line structure to the output file\n",
        "      out = {\"translation\": { src_id: src, trg_id: tgt } }\n",
        "      x = json.dumps(out, indent=0, ensure_ascii=False)\n",
        "      x = re.sub(r'\\n', ' ', x, 0, re.M)\n",
        "      out_json.write(x + \"\\n\")\n",
        "  out_json.close()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuaqXaq_M24N"
      },
      "outputs": [],
      "source": [
        "#we will use the FLORES 200 dev data for fine-tuning our mNMT model\n",
        "#https://github.com/facebookresearch/flores/tree/main/flores200\n",
        "train_src = \"eng_Latn.dev\" # English\n",
        "train_trg = \"cat_Latn.dev\" # Catalan\n",
        "train_json = \"en-cat.train.json\" #json file with source and target segments\n",
        "#call function to process the input files\n",
        "txt2json('eng_Latn', 'cat_Latn', train_src, train_trg, train_json)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune Multilingual NMT Model\n",
        "\n",
        "The following block contains the code to fine-tune the [NLLB](https://huggingface.co/docs/transformers/en/model_doc/nllb) model via [LoRA](https://huggingface.co/docs/transformers/en/peft) (Low-Rank Adaptation), which is an example of Parameter-Efficient Fine-Tuning (PEFT).\n",
        "\n",
        "Standard fine-tuning of a model as large as NLLB is computationally prohibitive, requiring significant GPU resources and storage. LoRA overcomes this challenge by focusing on the hypothesis that the 'update' required to specialize the pre-trained model for a new task (like our English-Catalan translation) within few parameters (matrices). By only training these new, small LoRA adapters (the low-rank matrices), we achieve effective transfer learning without the massive resource of updating every weight in the base NLLB model."
      ],
      "metadata": {
        "id": "3mclmlSOcpXO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7fC4fxTPMhb"
      },
      "outputs": [],
      "source": [
        "#import required libraries into code\n",
        "import torch\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_MAPPING,\n",
        "    AutoConfig,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    MBartTokenizer,\n",
        "    MBart50Tokenizer,\n",
        "    MBartTokenizerFast,\n",
        "    MBart50TokenizerFast,\n",
        "    M2M100Tokenizer,\n",
        "    SchedulerType,\n",
        "    default_data_collator,\n",
        "    get_scheduler,\n",
        "    set_seed,\n",
        ")\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from evaluate import load\n",
        "import transformers\n",
        "import os\n",
        "import numpy as np\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast, M2M100Tokenizer]\n",
        "model = None\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "def main(model_id,max_length, source_code,\n",
        "     target_code, forced_bos_token,\n",
        "     data_files, output_dir, train_bs,\n",
        "     grad_acc, lr, w_steps, n_epoch,\n",
        "     lr_scheduler_type,\n",
        "     lora_r, lora_alpha, lora_dropout):\n",
        "  \"\"\"\n",
        "  fine-tunes the model_id mNMT with given hyperparameters\n",
        "  \"\"\"\n",
        "    #loads NLLB tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    #loads NLLB model into the GPU\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id,\n",
        "                                                  device_map=\"auto\") #{\"\": 0}\n",
        "\n",
        "\n",
        "\n",
        "    # Set decoder_start_token_id\n",
        "    if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n",
        "      if isinstance(tokenizer, MBartTokenizer, MBartTokenizerFast):\n",
        "          model.config.decoder_start_token_id = tokenizer.lang_code_to_id[target_code]\n",
        "      else:\n",
        "          model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(target_code)\n",
        "\n",
        "    #set language id labels for the tokenizer, and for the beginning of the source sentence:\n",
        "    #[cat_Latn] The black cat.\n",
        "    if isinstance(tokenizer, tuple(MULTILINGUAL_TOKENIZERS)):\n",
        "\n",
        "      tokenizer.src_lang = source_code\n",
        "      tokenizer.tgt_lang = target_code\n",
        "\n",
        "      # For multilingual translation models like mBART-50 and M2M100 we need to force the target language token\n",
        "      # as the first generated token.\n",
        "      forced_bos_token_id = (\n",
        "          tokenizer.lang_code_to_id[forced_bos_token] if forced_bos_token is not None else None\n",
        "      )\n",
        "      model.config.forced_bos_token_id = forced_bos_token_id\n",
        "\n",
        "    #LoRA configuration:\n",
        "    #r is the intrinsic rank of the update matrices.\n",
        "    #Ranges from 4 to 256. Common starting points are 8, 16, or 32.\n",
        "    #A higher r increases model capacity but also computational cost.\n",
        "    #lora_alpha is the scaling factor applied to the adapted weights.\n",
        "    #It determines the influence of the LoRA updates relative to the original, frozen pre-trained weights\n",
        "    config = LoraConfig(r=lora_r,\n",
        "                        lora_alpha=lora_alpha,\n",
        "                        lora_dropout=lora_dropout, #0.05\n",
        "                        inference_mode=False,\n",
        "                        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "                        task_type=\"SEQ_2_SEQ_LM\")\n",
        "\n",
        "    model = get_peft_model(model, config)\n",
        "\n",
        "    print_trainable_parameters(model)\n",
        "    print(model)\n",
        "    #we use BLEU score for model selection\n",
        "    metric = load('sacrebleu', trust_remote_code=True)\n",
        "\n",
        "    def preprocess_parallel_function(examples):\n",
        "      \"\"\"\n",
        "      Loads json file into memory and tokenizes each segment\n",
        "      \"\"\"\n",
        "      inputs = [ex[source_code] for ex in examples[\"translation\"]]\n",
        "      targets = [ex[target_code] for ex in examples[\"translation\"]]\n",
        "\n",
        "      model_inputs = tokenizer(inputs, max_length=max_length, padding=False, truncation=True)\n",
        "\n",
        "\n",
        "      labels = tokenizer(targets, max_length=max_length, padding=False, truncation=True)\n",
        "\n",
        "      model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "      return model_inputs\n",
        "\n",
        "    def postprocess_text(preds, labels):\n",
        "      \"\"\"\n",
        "      prepares the model translations for the BLEU metric evaluation\n",
        "      \"\"\"\n",
        "      preds = [pred.strip() for pred in preds]\n",
        "      labels = [[label.strip()] for label in labels]\n",
        "\n",
        "      return preds, labels\n",
        "\n",
        "    def compute_metrics(eval_preds, ignore_pad_token_for_loss=False):\n",
        "      \"\"\"\n",
        "      computes the BLEU score for the model\n",
        "      \"\"\"\n",
        "      preds, labels = eval_preds\n",
        "      if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "      decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "      # Replace -100 in the labels as we can't decode them.\n",
        "      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "      # Some simple post-processing\n",
        "      decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "      result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "      prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "      result = {'bleu' : result['score']}\n",
        "      result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "      result = {k: round(v, 4) for k, v in result.items()}\n",
        "      return result\n",
        "\n",
        "    #loads data into memory\n",
        "    data = load_dataset(\"json\", data_files=data_files)\n",
        "    #splits data segments into training 90% and 10% validation for model selection\n",
        "    # seed for reproducibility\n",
        "    data_split = data['train'].train_test_split(test_size=0.1, seed=42)\n",
        "    data = DatasetDict({'train': data_split['train'],\n",
        "                        'valid': data_split['test']})\n",
        "\n",
        "\n",
        "    print(data)\n",
        "    #process data into batches of n segments at a time\n",
        "    data = data.map(preprocess_parallel_function,\n",
        "                    batched=True)\n",
        "    label_pad_token_id = -100\n",
        "    # model fine-tune configuration\n",
        "    trainer = transformers.Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        train_dataset=data[\"train\"],\n",
        "        eval_dataset=data[\"valid\"],\n",
        "        args=transformers.Seq2SeqTrainingArguments(\n",
        "            report_to='none', #turns off  wandb output visualization\n",
        "            per_device_train_batch_size=train_bs,\n",
        "            gradient_accumulation_steps=grad_acc,\n",
        "            do_train=True,\n",
        "            do_eval=True,\n",
        "            per_device_eval_batch_size=4,\n",
        "            eval_accumulation_steps=4,  #flushes inference memory from the gpu after 4 batches\n",
        "            warmup_ratio=w_steps,\n",
        "            lr_scheduler_type=lr_scheduler_type,\n",
        "            num_train_epochs=n_epoch,\n",
        "            predict_with_generate=True,\n",
        "            metric_for_best_model='bleu',\n",
        "            load_best_model_at_end=True,\n",
        "            learning_rate=lr,\n",
        "            save_total_limit=1,\n",
        "            save_strategy=\"epoch\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            output_dir=output_dir,\n",
        "        ),\n",
        "        data_collator=transformers.DataCollatorForSeq2Seq(tokenizer,\n",
        "                                                          label_pad_token_id=label_pad_token_id,\n",
        "                                                          model=model),\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    model.config.use_cache = False   # silence the warnings.\n",
        "    #executes training loop\n",
        "    trainer.train()\n",
        "    #saves fine-tuned model into output directory\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model hyper-parameters\n",
        "\n",
        "To perfom thw fine-tuning process and adapt the pre-trained NLLB model to our LRL pair, we must first define and configure several hyperparameters.\n",
        "\n",
        "Hyperparameters are external configuration variables set before the training process begins, guiding how the learning will occur. Their tuning is essential for achieving optimal model performance and resource efficiency.\n",
        "\n",
        "The main parameters we need to define include:\n",
        "\n",
        "- Learning Rate: This controls the magnitude of the weight updates during optimization. Setting it too high can lead to unstable training, while setting it too low can result in slow convergence.\n",
        "- Batch Size: This determines the number of training segments in one forward/backward pass. Larger batch sizes generally lead to more stable gradients but require significantly more GPU memory.\n",
        "- Learning Rate Scheduler: This defines the policy for dynamically adjusting the learning rate over the course of training (e.g., linear decay with warmup steps), which is critical for smooth and effective convergence in deep learning.\n",
        "- LoRA Configuration: Since we are using Parameter-Efficient Fine-Tuning (PEFT), we must also specify the LoRA-specific parameters, such as the Rank (r) and the Alpha, which control the model's capacity for adaptation and the strength of the updates."
      ],
      "metadata": {
        "id": "hmvQkR3UfIjV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rgHNp1MRgrc"
      },
      "outputs": [],
      "source": [
        "#fine-tune\n",
        "# we are going to use the NLLB version with 600M parameters\n",
        "model_id = \"facebook/nllb-200-distilled-600M\"\n",
        "#you can also use a bigger model: facebook/nllb-200-1.3B\n",
        "#double check the batch size\n",
        "#the max size of a segment is 128 tokens\n",
        "max_length = 128\n",
        "#we define the source and target iso id based on the NLLB format\n",
        "#for using different language pairs, please check:\n",
        "# https://github.com/facebookresearch/flores/tree/main/flores200\n",
        "source_code = 'eng_Latn' # English\n",
        "target_code = 'cat_Latn' #Catalan\n",
        "forced_bos_token = 'cat_Latn' #beggining of segment token\n",
        "data_files = \"en-cat.train.json\" #input  json file withsource and target segments\n",
        "output_dir = 'models/nllb-peft_finetune' #directory to save fine-tuned model\n",
        "train_bs = 12 #number of segments in a batch\n",
        "grad_acc = 1 #we wont use gradient accumulation, if the size of the batch is to big for gpu, reduce batch and increase gradient accumulation\n",
        "# for more info: https://huggingface.co/docs/accelerate/en/usage_guides/gradient_accumulation\n",
        "lr = 1e-4 #size of the lnearning rate\n",
        "w_steps = 0.03 #size of the warm-up steps\n",
        "n_epoch = 3  #number of epochs\n",
        "lr_scheduler_type = \"linear\" #type of learning rate sheduler\n",
        "lora_r = 16    #size r for LoRA\n",
        "lora_alpha = 32 #size alphra for LoRA\n",
        "lora_dropout = 0.1 #size of dropout for LoRA\n",
        "\n",
        "# fine-tune NLLB model\n",
        "main(model_id=model_id, max_length=max_length, source_code=source_code,\n",
        "     target_code=target_code, forced_bos_token=forced_bos_token,\n",
        "     data_files=data_files, output_dir=output_dir, train_bs=train_bs,\n",
        "     grad_acc=grad_acc, lr=lr, w_steps=w_steps, n_epoch=n_epoch,\n",
        "     lr_scheduler_type=lr_scheduler_type,\n",
        "     lora_r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# these code can free memory from the GPU if full\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "del model"
      ],
      "metadata": {
        "id": "5HiWlqmCkZpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilingual NMT Translation\n",
        "\n",
        "We will use the English-Catalan devtest split of the FLORES-200 benchmark as our test set. It's crucial that this data has not been used during any prior training or validation steps to ensure an unbiased measure of quiality.\n",
        "\n",
        "To facilitate translation, we will utilize the Hugging Face transformers pipeline. This allows us to rapidly generate Catalan translations for all English source segments using both models:\n",
        "\n",
        "The Baseline Model (the original, pre-trained NLLB model).\n",
        "\n",
        "The Fine-Tuned Model (the version adapted using LoRA on our specific LRL parallel corpus).\n",
        "\n",
        "By running both models on the identical test set, we establish a direct, comparison to assess the quality added by our fine-tuning process."
      ],
      "metadata": {
        "id": "31mN4LSEdLln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test translation with baseline NLLB model (inference)\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from peft import PeftModel, PeftConfig\n",
        "import codecs\n",
        "#output file\n",
        "mt_output_file = \"nllb.cat.txt\"\n",
        "mt_output = codecs.open(mt_output_file, 'w', 'utf-8')\n",
        "\n",
        "#inference with model, on GPU, and a given batch size\n",
        "mt_pipeline = pipeline(task=\"translation\",\n",
        "              model=\"facebook/nllb-200-distilled-600M\",\n",
        "              src_lang=\"eng_Latn\",\n",
        "              tgt_lang=\"cat_Latn\",\n",
        "              dtype=torch.float16,\n",
        "              device=0,\n",
        "              batch_size=6)\n",
        "\n",
        "#save translation into file\n",
        "# we use the English devtest file for evaluation\n",
        "file_name = \"eng_Latn.devtest\"\n",
        "print(mt_pipeline)\n",
        "with codecs.open(file_name, 'r', 'utf-8') as src:\n",
        "  lines = []\n",
        "  for line in src:\n",
        "    line = line.strip()\n",
        "    lines.append(line)\n",
        "print(lines)\n",
        "mt_lines = mt_pipeline(lines)\n",
        "for line in mt_lines:\n",
        "  print(line['translation_text'], file=mt_output)\n",
        "\n",
        "\n",
        "mt_output.close()"
      ],
      "metadata": {
        "id": "Z4rXPA_xpsDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5v6GaslTexz"
      },
      "outputs": [],
      "source": [
        "##Test translation with fine-tuned NLLB model (inference)\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "import codecs\n",
        "\n",
        "mt_output_file = \"nllb-peft_finetune_out.cat.txt\"\n",
        "mt_output = codecs.open(mt_output_file, 'w', 'utf-8')\n",
        "\n",
        "peft_model_id = \"models/nllb-peft_finetune\" #path of your directory with the fine-tuned model\n",
        "#load LoRA adapter\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id, src_lang=\"eng_Latn\")\n",
        "#print(config.base_model_name_or_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                              device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "file_name = \"eng_Latn.devtest\"\n",
        "#inference with fine-tuned model, on GPU, and a given batch size\n",
        "mt_pipeline = pipeline(task=\"translation\",\n",
        "              model=model,\n",
        "              tokenizer=tokenizer,\n",
        "              src_lang=\"eng_Latn\",\n",
        "              tgt_lang=\"cat_Latn\",\n",
        "              dtype=torch.float16,\n",
        "              batch_size=6)\n",
        "\n",
        "#save translation into file\n",
        "# we use the English devtest file for evaluation\n",
        "file_name = \"eng_Latn.devtest\"\n",
        "print(mt_pipeline)\n",
        "with codecs.open(file_name, 'r', 'utf-8') as src:\n",
        "  lines = []\n",
        "  for line in src:\n",
        "    line = line.strip()\n",
        "    lines.append(line)\n",
        "print(lines)\n",
        "mt_lines = mt_pipeline(lines)\n",
        "for line in mt_lines:\n",
        "  print(line['translation_text'], file=mt_output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mt_output.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation with Automatic Metrics\n",
        "\n",
        "For an evaluation of our multilingual NMT model's performance, we will rely on the industry-standard [sacreBLEU](https://github.com/mjpost/sacrebleu) library.\n",
        "\n",
        "We will use two metrics: BLEU (Bilingual Evaluation Understudy), which quantifies the n-gram overlap between the candidate and reference translations, and chrF (Character n-gram F-score), which is often a more reliable measure for morphologically rich languages (like Catalan) as it operates at the character level.\n",
        "\n",
        "Crucially, to determine if the performance difference observed between models is a significant improvement rather than random variation, we will employ bootstrapping (specifically, paired bootstrap resampling). This statistical technique allows us to test the difference in scores, enabling us to measure statistical significance and the confidence of whether one model outperforms the other."
      ],
      "metadata": {
        "id": "VPwG8tf1dobw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BLEU and chrf evaluation with baseline and fine-tuned model\n",
        "!sacrebleu cat_Latn.devtest -l en-ca -i nllb.cat.txt nllb-peft_finetune_out.cat.txt -f text -m bleu chrf --paired-bs"
      ],
      "metadata": {
        "id": "rFDdlVWq7DkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "\n",
        "- Beyond automatic metrics scores (e.g. BLEU, chrF), what specific differences in translation output would you expect to see between the baseline, and fine-tuned models, particularly in terms of accuracy, fluency, and terminology? For more information on manual evaluation, we direct the reader to **Chapter 10**.\n",
        "- How might the performance observed when comparing the baseline, and fine-tuned models shift when working with a LRL pair, and what unique challenges or opportunities might each approach present?\n",
        "- Considering the computational resources and data requirements associated with each approach (baseline, fine-tuning), given project constraints, how would you prioritise the use of one model over the others, based on automatic metrics scores?\n",
        "\n",
        "**NOTE**: please download the MT ouput text files (*.txt) for further comparison with the back-translation exercise. Use the file download buttom (3 dots next to a file) in the sidebar within the notebook interface."
      ],
      "metadata": {
        "id": "HSVWdgIWP0Ur"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}